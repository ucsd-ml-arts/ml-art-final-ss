{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A Basic Sentiment Analysis Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1fc47b6da20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pickle.load(open(\"SentimentRNN/merged_training.pkl\", 'rb'))\n",
    "data.emotions.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27383</th>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110083</th>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140764</th>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100071</th>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>i beleive that i am much more sensitive to oth...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text emotions\n",
       "27383   i feel awful about it too because it s my job ...  sadness\n",
       "110083                              im alone i feel awful  sadness\n",
       "140764  ive probably mentioned this before but i reall...      joy\n",
       "100071           i was feeling a little low few days back  sadness\n",
       "2837    i beleive that i am much more sensitive to oth...     love"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only text that contain less that 70 tokens to avoid too much padding\n",
    "data[\"token_size\"] = data[\"text\"].apply(lambda x: len(x.split(' ')))\n",
    "data = data.loc[data['token_size'] < 70].copy()\n",
    "\n",
    "# sampling\n",
    "data = data.sample(n=50000, random_state=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "# (e.g., 5 -> \"dad\") for the dataset\n",
    "class ConstructVocab():\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_index()\n",
    "        \n",
    "    def create_index(self):\n",
    "        for s in self.sentences:\n",
    "            # update with individual tokens\n",
    "            self.vocab.update(s.split(' '))\n",
    "            \n",
    "        # sort the vocab\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        # add a padding token with index 0\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        \n",
    "        # word to index mapping\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1 # +1 because of pad token\n",
    "        \n",
    "        # index to word mapping\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word\n",
    "            \n",
    "# construct vocab\n",
    "inputs = ConstructVocab(data[\"text\"].values.tolist())\n",
    "input_tensor = [[inputs.word2idx[s] for s in es.split(' ')]  for es in data[\"text\"].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this encoder for use later\n",
    "TEXT_ENCODER_PATH = './SentimentRNN/textEncoder.pkl'\n",
    "\n",
    "with open(TEXT_ENCODER_PATH, 'wb') as encoderPath:\n",
    "    pickle.dump(inputs, encoderPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad data\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "max_length_inp = max_length(input_tensor)\n",
    "input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                             maxlen=max_length_inp,\n",
    "                                                             padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anger'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### convert targets to one-hot encoding vectors\n",
    "emotions = list(set(data.emotions.unique()))\n",
    "num_emotions = len(emotions)\n",
    "\n",
    "# binarizer\n",
    "mlb = preprocessing.MultiLabelBinarizer()\n",
    "data_labels =  [set(emos) & set(emotions) for emos in data[['emotions']].values]\n",
    "bin_emotions = mlb.fit_transform(data_labels)\n",
    "target_tensor = np.array(bin_emotions.tolist())\n",
    "\n",
    "# eval dicts\n",
    "get_emotion = lambda t: np.argmax(t)\n",
    "emotion_dict = {0: 'anger', 1: 'fear', 2: 'joy', 3: 'love', 4: 'sadness', 5: 'surprise'}\n",
    "\n",
    "# ie.\n",
    "emotion_dict[get_emotion(target_tensor[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 40000, 5000, 5000, 5000, 5000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Split the validataion further to obtain a holdout dataset (for testing) -- split 50:50\n",
    "input_tensor_val, input_tensor_test, target_tensor_val, target_tensor_test = train_test_split(input_tensor_val, target_tensor_val, test_size=0.5)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val), len(input_tensor_test), len(target_tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27511\n",
      "<DatasetV1Adapter shapes: ((64, 68), (64, 6)), types: (tf.int32, tf.int32)>\n",
      "<DatasetV1Adapter shapes: ((64, 68), (64, 6)), types: (tf.int32, tf.int32)>\n",
      "<DatasetV1Adapter shapes: ((64, 68), (64, 6)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BUFFER_SIZE = len(input_tensor_train)\n",
    "VAL_BUFFER_SIZE = len(input_tensor_val)\n",
    "TEST_BUFFER_SIZE = len(input_tensor_test)\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_N_BATCH = TRAIN_BUFFER_SIZE // BATCH_SIZE\n",
    "VAL_N_BATCH = VAL_BUFFER_SIZE // BATCH_SIZE\n",
    "TEST_N_BATCH = TEST_BUFFER_SIZE // BATCH_SIZE\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inputs.word2idx)\n",
    "print(vocab_inp_size)\n",
    "target_size = num_emotions\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, \n",
    "                                                    target_tensor_train)).shuffle(TRAIN_BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, \n",
    "                                                  target_tensor_val)).shuffle(VAL_BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_test, \n",
    "                                                    target_tensor_test)).shuffle(TEST_BUFFER_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# checking minibatch\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define the GRU component\n",
    "def gru(units):\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True, \n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='relu', \n",
    "                               recurrent_initializer='glorot_uniform')\n",
    "\n",
    "### Build the model\n",
    "class EmoGRU(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_units, batch_sz, output_size):\n",
    "        super(EmoGRU, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        # layers\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "        self.gru = gru(self.hidden_units)\n",
    "        self.fc = tf.keras.layers.Dense(output_size)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x) # batch_size X max_len X embedding_dim\n",
    "        output, state = self.gru(x, initial_state = hidden) #  batch_size X max_len X hidden_units\n",
    "        out = output[:,-1,:]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out) # batch_size X max_len X output_size\n",
    "        return out, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.hidden_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\brian\\.conda\\envs\\conda_env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "(64, 6)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  7042816   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru (CuDNNGRU)         multiple                  3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  6150      \n",
      "=================================================================\n",
      "Total params: 10,987,270\n",
      "Trainable params: 10,987,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = EmoGRU(vocab_inp_size, embedding_dim, units, BATCH_SIZE, target_size)\n",
    "\n",
    "# initialize the hidden state of the RNN\n",
    "hidden = model.initialize_hidden_state()\n",
    "\n",
    "# testing for the first batch only then break the for loop\n",
    "# Potential bug: out is not randomized enough\n",
    "for (batch, (inp, targ)) in enumerate(train_dataset):\n",
    "    out, state = model(inp, hidden)\n",
    "    print(out.shape) \n",
    "    break\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "def loss_function(y, prediction):\n",
    "    return tf.losses.softmax_cross_entropy(y, logits=prediction)\n",
    "\n",
    "def accuracy(y, yhat):\n",
    "    #compare the predictions to the truth\n",
    "    yhat = tf.argmax(yhat, 1).numpy()\n",
    "    y    = tf.argmax(y   , 1).numpy()\n",
    "    return np.sum(y == yhat)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\brian\\.conda\\envs\\conda_env\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1 Batch 0 Val. Loss 0.2991\n",
      "Epoch 1 Batch 100 Val. Loss 0.2490\n",
      "Epoch 1 Batch 200 Val. Loss 0.2654\n",
      "Epoch 1 Batch 300 Val. Loss 0.2692\n",
      "Epoch 1 Batch 400 Val. Loss 0.2636\n",
      "Epoch 1 Batch 500 Val. Loss 0.2585\n",
      "Epoch 1 Batch 600 Val. Loss 0.2200\n",
      "Epoch 1 Loss 0.2638 -- Train Acc. 0.3315 -- Val Acc. 0.2937\n",
      "Time taken for 1 epoch 93.34416127204895 sec\n",
      "\n",
      "Epoch 2 Batch 0 Val. Loss 0.2644\n",
      "Epoch 2 Batch 100 Val. Loss 0.2584\n",
      "Epoch 2 Batch 200 Val. Loss 0.2385\n",
      "Epoch 2 Batch 300 Val. Loss 0.2429\n",
      "Epoch 2 Batch 400 Val. Loss 0.2312\n",
      "Epoch 2 Batch 500 Val. Loss 0.2283\n",
      "Epoch 2 Batch 600 Val. Loss 0.2231\n",
      "Epoch 2 Loss 0.2481 -- Train Acc. 0.4124 -- Val Acc. 0.5409\n",
      "Time taken for 1 epoch 81.85736632347107 sec\n",
      "\n",
      "Epoch 3 Batch 0 Val. Loss 0.2529\n",
      "Epoch 3 Batch 100 Val. Loss 0.1848\n",
      "Epoch 3 Batch 200 Val. Loss 0.2100\n",
      "Epoch 3 Batch 300 Val. Loss 0.1869\n",
      "Epoch 3 Batch 400 Val. Loss 0.1646\n",
      "Epoch 3 Batch 500 Val. Loss 0.2028\n",
      "Epoch 3 Batch 600 Val. Loss 0.1520\n",
      "Epoch 3 Loss 0.1870 -- Train Acc. 0.5944 -- Val Acc. 0.6268\n",
      "Time taken for 1 epoch 86.35013389587402 sec\n",
      "\n",
      "Epoch 4 Batch 0 Val. Loss 0.1647\n",
      "Epoch 4 Batch 100 Val. Loss 0.1469\n",
      "Epoch 4 Batch 200 Val. Loss 0.1665\n",
      "Epoch 4 Batch 300 Val. Loss 0.1755\n",
      "Epoch 4 Batch 400 Val. Loss 0.1232\n",
      "Epoch 4 Batch 500 Val. Loss 0.1174\n",
      "Epoch 4 Batch 600 Val. Loss 0.1085\n",
      "Epoch 4 Loss 0.1508 -- Train Acc. 0.6823 -- Val Acc. 0.7027\n",
      "Time taken for 1 epoch 97.76607155799866 sec\n",
      "\n",
      "Epoch 5 Batch 0 Val. Loss 0.1532\n",
      "Epoch 5 Batch 100 Val. Loss 0.1437\n",
      "Epoch 5 Batch 200 Val. Loss 0.1336\n",
      "Epoch 5 Batch 300 Val. Loss 0.1033\n",
      "Epoch 5 Batch 400 Val. Loss 0.1271\n",
      "Epoch 5 Batch 500 Val. Loss 0.1141\n",
      "Epoch 5 Batch 600 Val. Loss 0.0968\n",
      "Epoch 5 Loss 0.1184 -- Train Acc. 0.7534 -- Val Acc. 0.7480\n",
      "Time taken for 1 epoch 101.17011952400208 sec\n",
      "\n",
      "Epoch 6 Batch 0 Val. Loss 0.1122\n",
      "Epoch 6 Batch 100 Val. Loss 0.0753\n",
      "Epoch 6 Batch 200 Val. Loss 0.0582\n",
      "Epoch 6 Batch 300 Val. Loss 0.0840\n",
      "Epoch 6 Batch 400 Val. Loss 0.0927\n",
      "Epoch 6 Batch 500 Val. Loss 0.1150\n",
      "Epoch 6 Batch 600 Val. Loss 0.0810\n",
      "Epoch 6 Loss 0.0962 -- Train Acc. 0.8021 -- Val Acc. 0.7690\n",
      "Time taken for 1 epoch 114.10034418106079 sec\n",
      "\n",
      "Epoch 7 Batch 0 Val. Loss 0.1049\n",
      "Epoch 7 Batch 100 Val. Loss 0.0644\n",
      "Epoch 7 Batch 200 Val. Loss 0.0802\n",
      "Epoch 7 Batch 300 Val. Loss 0.0884\n",
      "Epoch 7 Batch 400 Val. Loss 0.1107\n",
      "Epoch 7 Batch 500 Val. Loss 0.0655\n",
      "Epoch 7 Batch 600 Val. Loss 0.0880\n",
      "Epoch 7 Loss 0.0809 -- Train Acc. 0.8321 -- Val Acc. 0.7893\n",
      "Time taken for 1 epoch 114.958425283432 sec\n",
      "\n",
      "Epoch 8 Batch 0 Val. Loss 0.0938\n",
      "Epoch 8 Batch 100 Val. Loss 0.0758\n",
      "Epoch 8 Batch 200 Val. Loss 0.0536\n",
      "Epoch 8 Batch 300 Val. Loss 0.0704\n",
      "Epoch 8 Batch 400 Val. Loss 0.0818\n",
      "Epoch 8 Batch 500 Val. Loss 0.0805\n",
      "Epoch 8 Batch 600 Val. Loss 0.0547\n",
      "Epoch 8 Loss 0.0706 -- Train Acc. 0.8540 -- Val Acc. 0.8001\n",
      "Time taken for 1 epoch 100.12256693840027 sec\n",
      "\n",
      "Epoch 9 Batch 0 Val. Loss 0.0789\n",
      "Epoch 9 Batch 100 Val. Loss 0.0490\n",
      "Epoch 9 Batch 200 Val. Loss 0.0725\n",
      "Epoch 9 Batch 300 Val. Loss 0.0565\n",
      "Epoch 9 Batch 400 Val. Loss 0.0519\n",
      "Epoch 9 Batch 500 Val. Loss 0.0556\n",
      "Epoch 9 Batch 600 Val. Loss 0.0518\n",
      "Epoch 9 Loss 0.0615 -- Train Acc. 0.8728 -- Val Acc. 0.8109\n",
      "Time taken for 1 epoch 93.52576756477356 sec\n",
      "\n",
      "Epoch 10 Batch 0 Val. Loss 0.0554\n",
      "Epoch 10 Batch 100 Val. Loss 0.0527\n",
      "Epoch 10 Batch 200 Val. Loss 0.0433\n",
      "Epoch 10 Batch 300 Val. Loss 0.0545\n",
      "Epoch 10 Batch 400 Val. Loss 0.0412\n",
      "Epoch 10 Batch 500 Val. Loss 0.0648\n",
      "Epoch 10 Batch 600 Val. Loss 0.0482\n",
      "Epoch 10 Loss 0.0536 -- Train Acc. 0.8880 -- Val Acc. 0.8225\n",
      "Time taken for 1 epoch 91.7094144821167 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    ### Initialize hidden state\n",
    "    hidden = model.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    train_accuracy, val_accuracy = 0, 0\n",
    "    \n",
    "    ### Training\n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions,_ = model(inp, hidden)\n",
    "            loss += loss_function(targ, predictions)\n",
    "        batch_loss = (loss / int(targ.shape[1]))        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        batch_accuracy = accuracy(targ, predictions)\n",
    "        train_accuracy += batch_accuracy\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Val. Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "            \n",
    "    ### Validating\n",
    "    hidden = model.initialize_hidden_state()\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(val_dataset):        \n",
    "        predictions,_ = model(inp, hidden)        \n",
    "        batch_accuracy = accuracy(targ, predictions)\n",
    "        val_accuracy += batch_accuracy\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f} -- Train Acc. {:.4f} -- Val Acc. {:.4f}'.format(epoch + 1, \n",
    "                                                             total_loss / TRAIN_N_BATCH, \n",
    "                                                             train_accuracy / TRAIN_N_BATCH,\n",
    "                                                             val_accuracy / VAL_N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.8137019230769231\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on testing data\n",
    "test_accuracy = 0\n",
    "all_predictions = []\n",
    "x_raw = []\n",
    "y_raw = []\n",
    "\n",
    "# need fresh model hidden state only per batch???\n",
    "hidden = model.initialize_hidden_state()\n",
    "\n",
    "for (batch, (inp, targ)) in enumerate(test_dataset):\n",
    "    predictions,_ = model(inp, hidden)\n",
    "    batch_accuracy = accuracy(targ, predictions)\n",
    "    test_accuracy += batch_accuracy\n",
    "    \n",
    "    x_raw = x_raw + [x for x in inp]\n",
    "    y_raw = y_raw + [y for y in targ]\n",
    "    \n",
    "    all_predictions.append(predictions)\n",
    "    \n",
    "print(\"Test Accuracy: \", test_accuracy/TEST_N_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\brian\\.conda\\envs\\conda_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n"
     ]
    }
   ],
   "source": [
    "# Save model for usage\n",
    "model.save_weights('SentimentRNN/checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
